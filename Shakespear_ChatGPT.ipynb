{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOr3fblAS0QaM2k1x/GpSSF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moutard/llm-practice/blob/main/Shakespear_ChatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load DataSet\n",
        "Shakespeare dataset"
      ],
      "metadata": {
        "id": "QF-6-bsDepSM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXvTXeo0IpAL",
        "outputId": "e6949ad1-5e60-43ad-8f71-eac0ef5eec3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-14 06:52:46--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2024-05-14 06:52:46 (173 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7GjM5Fnl8By",
        "outputId": "453ecb1c-95af-49fb-ba22-484b471c0e9e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d4252e3b0b0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as shakespeare_file:\n",
        "    shakespeare_dataset = shakespeare_file.read()\n",
        "    print(\"length of dataset in characters:\", len(shakespeare_dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DevDih_bJHAH",
        "outputId": "2abbd381-a350-4444-984a-22c89ec53448"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters (Global variables)"
      ],
      "metadata": {
        "id": "QDkBSY57cXZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMDEDDING_DIMENSION = 32 # size of the embedding that encodes a token\n",
        "BATCH_SIZE = 32\n",
        "BLOCK_SIZE = 8\n",
        "MAX_ITER = 5000\n",
        "EVAL_INTERVAL = 500\n",
        "EVAL_ITERATIONS = 200\n",
        "LEARNING_RATE = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "5RqRQXW3cU2N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract the Char Set"
      ],
      "metadata": {
        "id": "QgFZEN8BJZAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "charset = sorted(list(set(shakespeare_dataset)))\n",
        "vocab_size = len(charset)\n",
        "print(charset)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPSvlZF9KA61",
        "outputId": "dff4386c-4c0f-4cc8-8e4f-9414b4e76831"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encode_letter = lambda char : charset.index(char)\n",
        "decode_letter = lambda index : charset[index]\n",
        "encode_string = lambda text : [encode_letter(char) for i, char in enumerate(text)] # need to take into account when the char input not present in the original dataset\n",
        "decode_string = lambda list_of_indexes : [decode_letter(index) for index in list_of_indexes]\n",
        "\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(charset) }\n",
        "itos = { i:ch for i,ch in enumerate(charset) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "\n",
        "# openAI use ticktoken\n",
        "# Google uses"
      ],
      "metadata": {
        "id": "jErWTRICOBPS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"hello world\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2K3-aa4PKVL",
        "outputId": "cb63cae9-1fcf-40cc-d64a-b352e7c53ba1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_tensor = torch.tensor(encode(shakespeare_dataset), dtype=torch.long)"
      ],
      "metadata": {
        "id": "33hqwakQQNj4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = shakespeare_tensor[:int(0.9*len(shakespeare_tensor))]\n",
        "val_data = shakespeare_tensor[int(0.9*len(shakespeare_tensor)):]\n",
        "\n",
        "example_training_batch = train_data[:BLOCK_SIZE]\n",
        "example_target_batch = train_data[1:BLOCK_SIZE+1]\n",
        "\n",
        "for i in range(BLOCK_SIZE):\n",
        "  context = example_training_batch[:i]\n",
        "  target = example_target_batch[i]\n",
        "  print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al5m9sU8RP0e",
        "outputId": "c83699ce-7ebb-4093-a53c-78ff10b81203"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([], dtype=torch.int64) the target: 47\n",
            "when input is tensor([18]) the target: 56\n",
            "when input is tensor([18, 47]) the target: 57\n",
            "when input is tensor([18, 47, 56]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = BATCH_SIZE # how many independent sequences will we process in parallel?\n",
        "block_size = BLOCK_SIZE # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print(xb.shape, yb.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr2jX3ueRhOL",
        "outputId": "cefa2964-c59e-4dd3-afea-93aa74001a6d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 8]) torch.Size([32, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  \"\"\"\n",
        "  one head of self-attention\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, head_size, embedding_dimension, block_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(embedding_dimension, head_size, bias=False)\n",
        "    self.query = nn.Linear(embedding_dimension, head_size, bias=False)\n",
        "    self.value = nn.Linear(embedding_dimension, head_size, bias=False)\n",
        "    # where is block_size defined?\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # input of size (batch, time-step, channels)\n",
        "    # output of size (batch, time-step, head size)\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x) # (B, T, 16)\n",
        "    q = self.query (x) # (B, T, 16)\n",
        "    wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
        "    # query what I am looking for and key what I contain\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # you don't want future tokens to impact the current token\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "    out = wei @ x\n",
        "    return out # (B, T, T) @ (B, T, C)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t0qT0GutcTGM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, num_heads, head_size, embedding_dimension, block_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size, embedding_dimension, block_size) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n"
      ],
      "metadata": {
        "id": "5T08_g3Fc9On"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bigram language model\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dimension=EMDEDDING_DIMENSION, block_size=BLOCK_SIZE):\n",
        "    super().__init__()\n",
        "    # each token directly read off the logits\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, embedding_dimension)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, embedding_dimension)\n",
        "    import pdb\n",
        "    pdb.set_trace()\n",
        "    self.sa_heads = MultiHeadAttention(4, embedding_dimension/4, embedding_dimension, block_size) # 4 heads of 8-dimensional self-attention\n",
        "    # self.self_attention_head = Head(embedding_dimension, embedding_dimension, block_size)\n",
        "    self.lm_head = nn.Linear(embedding_dimension, vocab_size)\n",
        "\n",
        "  \"\"\"\n",
        "    idx:\n",
        "     and targets are both (B,T) tensor of integers\n",
        "  \"\"\"\n",
        "  def forward(self, idx: torch.Tensor, targets=None):\n",
        "    B, T = idx.shape\n",
        "    # encore the idx\n",
        "    token_embeddings = self.token_embedding_table(idx) # (B,T,C) (Batch size, Time, Channels = vocab_size)\n",
        "\n",
        "    # encore position of idx\n",
        "    position_embeddings = self.position_embedding_table(torch.arange(T)) # (T, C)\n",
        "    # logits are scores based on the token.\n",
        "    x = token_embeddings + position_embeddings # (B, T, C)\n",
        "    # Rq: for now the position doesn't matter because we are using a bigram\n",
        "    x = self.sa_heads(x) # apply one head of self-attention of (B, T, C)\n",
        "\n",
        "\n",
        "    logits = self.lm_head(token_embeddings) # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "        loss = None\n",
        "    else:\n",
        "        B, T, C = logits.shape\n",
        "        # the Channels C needs to the the second dimension expecting B * C *T for F.cross_entropy\n",
        "        logits = logits.view(B*T, C)\n",
        "        targets = targets.view(B*T)\n",
        "        # negative cross livelyhood (using it's functional form so we don't need to create a module for it)\n",
        "        # compare how good we predict logits vs targets\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  \"\"\"\n",
        "    idx: current context by batch\n",
        "    max_new_tokens : number of token to generate\n",
        "  \"\"\"\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "        # crop idx to the last block_size tokens\n",
        "        idx_cond = idx[:, -BLOCK_SIZE:]\n",
        "        # get the predictions\n",
        "        logits, loss = self(idx_cond) # get to the forward option (so targets can be None, and no need to create a loss)\n",
        "        # focus only on the last time step\n",
        "        logits = logits[:, -1, :] # becomes (B, C)\n",
        "        # apply softmax to get probabilities\n",
        "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "        # sample from the distribution\n",
        "        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "        # append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "out,loss = m(xb, yb)\n",
        "print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TAM_QF524DVD",
        "outputId": "1ef4b487-44c9-4259-c0cd-487e684e3cf1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/bdb.py\", line 336, in set_trace\n",
            "    sys.settrace(self.trace_dispatch)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> \u001b[0;32m<ipython-input-22-5cc0f0995cf6>\u001b[0m(12)\u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     10 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     11 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 12 \u001b[0;31m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dimension\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 4 heads of 8-dimensional self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     13 \u001b[0;31m    \u001b[0;31m# self.self_attention_head = Head(embedding_dimension, embedding_dimension, block_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     14 \u001b[0;31m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> embedding_dimension\n",
            "32\n",
            "ipdb> block_size\n",
            "8\n",
            "ipdb> n\n",
            "TypeError: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n",
            " * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
            " * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
            "> \u001b[0;32m<ipython-input-22-5cc0f0995cf6>\u001b[0m(12)\u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     10 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     11 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 12 \u001b[0;31m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dimension\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 4 heads of 8-dimensional self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     13 \u001b[0;31m    \u001b[0;31m# self.self_attention_head = Head(embedding_dimension, embedding_dimension, block_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     14 \u001b[0;31m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "--Return--\n",
            "None\n",
            "> \u001b[0;32m<ipython-input-22-5cc0f0995cf6>\u001b[0m(12)\u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     10 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     11 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 12 \u001b[0;31m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dimension\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 4 heads of 8-dimensional self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     13 \u001b[0;31m    \u001b[0;31m# self.self_attention_head = Head(embedding_dimension, embedding_dimension, block_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     14 \u001b[0;31m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "TypeError: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n",
            " * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
            " * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
            "> \u001b[0;32m<ipython-input-22-5cc0f0995cf6>\u001b[0m(69)\u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     67 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     68 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 69 \u001b[0;31m\u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBigramLanguageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     70 \u001b[0;31m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     71 \u001b[0;31m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/bdb.py\", line 361, in set_quit\n",
            "    sys.settrace(None)\n",
            "\n",
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/debugger.py\", line 1075, in cmdloop\n",
            "    sys.settrace(None)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--KeyboardInterrupt--\n",
            "\n",
            "KeyboardInterrupt: Interrupted by user\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-5cc0f0995cf6>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBigramLanguageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-5cc0f0995cf6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, embedding_dimension, block_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dimension\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 4 heads of 8-dimensional self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# self.self_attention_head = Head(embedding_dimension, embedding_dimension, block_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-80aa731edcf6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_heads, head_size, embedding_dimension, block_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mHead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-80aa731edcf6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mHead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-130ae786f6b3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, head_size, embedding_dimension, block_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- what's the difference between self-attention and cross-attention. Self-attention all come from x (k and q both come from x. cross-attention where k comes from x and q comes from another vector (for instance for translation we use cross-attention where k where from the target language and q the original language)\n",
        "\n",
        "- wei need to be diffuse so it needs to be divided by squared root of head_size\n",
        "` * head_size**-0.5` if you don't do that, the variance is going to change."
      ],
      "metadata": {
        "id": "Gi1VEwURi7dM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch Optimizer\n",
        "optimiser = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "d-087cqDJwNC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for steps in range(EVAL_ITERATIONS):\n",
        "  # sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "  # evaluate the loss\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimiser.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimiser.step()\n",
        "  print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbOGl3xDOSIz",
        "outputId": "c8ecc270-f74d-47ca-afc9-d4211838a3fb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.376278400421143\n",
            "4.321561336517334\n",
            "4.254098892211914\n",
            "4.295942306518555\n",
            "4.269122123718262\n",
            "4.283777236938477\n",
            "4.235698223114014\n",
            "4.2078142166137695\n",
            "4.2039361000061035\n",
            "4.286467552185059\n",
            "4.240995407104492\n",
            "4.186239242553711\n",
            "4.265632152557373\n",
            "4.216305255889893\n",
            "4.211254596710205\n",
            "4.123067855834961\n",
            "4.193725109100342\n",
            "4.142827987670898\n",
            "4.150857448577881\n",
            "4.180758953094482\n",
            "4.117696285247803\n",
            "4.097591400146484\n",
            "4.171906471252441\n",
            "4.084340572357178\n",
            "4.112943172454834\n",
            "4.146841049194336\n",
            "4.0683393478393555\n",
            "4.072709083557129\n",
            "4.102790355682373\n",
            "4.0087504386901855\n",
            "4.063634872436523\n",
            "4.007064342498779\n",
            "4.080782413482666\n",
            "4.020970821380615\n",
            "3.9313879013061523\n",
            "4.007304668426514\n",
            "4.005529403686523\n",
            "3.9916744232177734\n",
            "4.052687644958496\n",
            "3.914335250854492\n",
            "3.9284307956695557\n",
            "3.981619119644165\n",
            "4.011439323425293\n",
            "3.9609692096710205\n",
            "3.950890064239502\n",
            "4.0291972160339355\n",
            "3.8926281929016113\n",
            "3.878202199935913\n",
            "3.9470181465148926\n",
            "3.8863375186920166\n",
            "3.86076283454895\n",
            "3.8825433254241943\n",
            "3.9164633750915527\n",
            "3.93204927444458\n",
            "3.9118309020996094\n",
            "3.9240787029266357\n",
            "3.831287384033203\n",
            "3.8841652870178223\n",
            "3.8798646926879883\n",
            "3.9054057598114014\n",
            "3.8428900241851807\n",
            "3.8553786277770996\n",
            "3.841482400894165\n",
            "3.7911064624786377\n",
            "3.8546252250671387\n",
            "3.794306993484497\n",
            "3.877913475036621\n",
            "3.778733491897583\n",
            "3.8267974853515625\n",
            "3.730351209640503\n",
            "3.7370197772979736\n",
            "3.6532297134399414\n",
            "3.8322155475616455\n",
            "3.811455011367798\n",
            "3.737668752670288\n",
            "3.550361156463623\n",
            "3.7750957012176514\n",
            "3.6589221954345703\n",
            "3.697188138961792\n",
            "3.6731653213500977\n",
            "3.6840903759002686\n",
            "3.732248544692993\n",
            "3.667372703552246\n",
            "3.68782377243042\n",
            "3.771331310272217\n",
            "3.7368733882904053\n",
            "3.7928431034088135\n",
            "3.5926382541656494\n",
            "3.669382095336914\n",
            "3.640934944152832\n",
            "3.6888539791107178\n",
            "3.5642166137695312\n",
            "3.719939947128296\n",
            "3.627483606338501\n",
            "3.7391467094421387\n",
            "3.5798983573913574\n",
            "3.600900650024414\n",
            "3.5731892585754395\n",
            "3.6791751384735107\n",
            "3.6076772212982178\n",
            "3.6221060752868652\n",
            "3.5280425548553467\n",
            "3.6284406185150146\n",
            "3.5837454795837402\n",
            "3.5494937896728516\n",
            "3.4935860633850098\n",
            "3.4600918292999268\n",
            "3.5424697399139404\n",
            "3.464796304702759\n",
            "3.5495123863220215\n",
            "3.6125476360321045\n",
            "3.5307960510253906\n",
            "3.582068681716919\n",
            "3.560581922531128\n",
            "3.494638204574585\n",
            "3.493293046951294\n",
            "3.493943929672241\n",
            "3.6144726276397705\n",
            "3.528228282928467\n",
            "3.3868677616119385\n",
            "3.526233434677124\n",
            "3.3986005783081055\n",
            "3.4861018657684326\n",
            "3.5279605388641357\n",
            "3.5559897422790527\n",
            "3.385579824447632\n",
            "3.444791793823242\n",
            "3.4527482986450195\n",
            "3.4436380863189697\n",
            "3.5032410621643066\n",
            "3.438279628753662\n",
            "3.450512647628784\n",
            "3.2979438304901123\n",
            "3.4034221172332764\n",
            "3.4903829097747803\n",
            "3.410980701446533\n",
            "3.1293933391571045\n",
            "3.378248453140259\n",
            "3.34926176071167\n",
            "3.4486546516418457\n",
            "3.2874128818511963\n",
            "3.4511845111846924\n",
            "3.5881192684173584\n",
            "3.4271163940429688\n",
            "3.491386651992798\n",
            "3.42606520652771\n",
            "3.4031131267547607\n",
            "3.334765672683716\n",
            "3.394198179244995\n",
            "3.2984046936035156\n",
            "3.263627290725708\n",
            "3.38169527053833\n",
            "3.3319947719573975\n",
            "3.297104835510254\n",
            "3.35848331451416\n",
            "3.473815679550171\n",
            "3.412330150604248\n",
            "3.4303574562072754\n",
            "3.2643027305603027\n",
            "3.4263150691986084\n",
            "3.2467739582061768\n",
            "3.2008261680603027\n",
            "3.3504228591918945\n",
            "3.2380921840667725\n",
            "3.175407886505127\n",
            "3.158219337463379\n",
            "3.326587438583374\n",
            "3.122274160385132\n",
            "3.1400911808013916\n",
            "3.1466171741485596\n",
            "3.1202023029327393\n",
            "3.1864447593688965\n",
            "3.188547372817993\n",
            "3.2517049312591553\n",
            "3.314028263092041\n",
            "3.17708158493042\n",
            "3.3822438716888428\n",
            "3.162501096725464\n",
            "3.246939182281494\n",
            "3.182847738265991\n",
            "3.2505600452423096\n",
            "3.2236132621765137\n",
            "3.0465846061706543\n",
            "3.1084086894989014\n",
            "3.236414909362793\n",
            "3.2413175106048584\n",
            "3.10591197013855\n",
            "3.072089195251465\n",
            "3.1128973960876465\n",
            "3.148483991622925\n",
            "3.2197203636169434\n",
            "3.151359796524048\n",
            "3.1058297157287598\n",
            "3.1585679054260254\n",
            "3.187636375427246\n",
            "3.128176689147949\n",
            "3.148472547531128\n",
            "3.1336989402770996\n",
            "3.0250070095062256\n",
            "3.101811408996582\n"
          ]
        }
      ]
    }
  ]
}